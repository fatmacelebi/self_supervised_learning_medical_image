{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c68c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616966b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3342b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install wandb for experiment tracking\n",
    "!pip install --upgrade https://github.com/wandb/client/archive/feature/code-save.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1926b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013ae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other imports\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Random seed fixation\n",
    "tf.random.set_seed(666)\n",
    "np.random.seed(666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1195049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many training images for SimCLR?\n",
    "train_images = list(paths.list_images(\"path_for_unlabaled_train_images/\"))\n",
    "print(len(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391eb75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation utilities (differs from the original implementation)\n",
    "# Referred from: https://arxiv.org/pdf/2002.05709.pdf (Appendxi A \n",
    "# corresponding GitHub: https://github.com/google-research/simclr/)\n",
    "\n",
    "class CustomAugment(object):\n",
    "    def __call__(self, sample):        \n",
    "        # Random flips\n",
    "        sample = self._random_apply(tf.image.flip_left_right, sample, p=0.5)\n",
    "        \n",
    "        # Randomly apply transformation (color distortions) with probability p.\n",
    "        sample = self._random_apply(self._color_jitter, sample, p=0.8)\n",
    "        sample = self._random_apply(self._color_drop, sample, p=0.2)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def _color_jitter(self, x, s=1):\n",
    "        # one can also shuffle the order of following augmentations\n",
    "        # each time they are applied.\n",
    "        x = tf.image.random_brightness(x, max_delta=0.8*s)\n",
    "        x = tf.image.random_contrast(x, lower=1-0.8*s, upper=1+0.8*s)\n",
    "        x = tf.image.random_saturation(x, lower=1-0.8*s, upper=1+0.8*s)\n",
    "        x = tf.image.random_hue(x, max_delta=0.2*s)\n",
    "        x = tf.clip_by_value(x, 0, 1)\n",
    "        return x\n",
    "    \n",
    "    def _color_drop(self, x):\n",
    "        x = tf.image.rgb_to_grayscale(x)\n",
    "        x = tf.tile(x, [1, 1, 1, 3])\n",
    "        return x\n",
    "    \n",
    "    def _random_apply(self, func, x, p):\n",
    "        return tf.cond(\n",
    "          tf.less(tf.random.uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
    "                  tf.cast(p, tf.float32)),\n",
    "          lambda: func(x),\n",
    "          lambda: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63483bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the augmentation pipeline\n",
    "data_augmentation = Sequential([Lambda(CustomAugment())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37beea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing utils\n",
    "@tf.function\n",
    "def parse_images(image_path):\n",
    "    image_string = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, size=[224, 224])\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa201da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow dataset\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(train_images)\n",
    "train_ds = (\n",
    "    train_ds\n",
    "    .map(parse_images, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .shuffle(1024)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265d12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture utils\n",
    "def get_resnet_simclr(hidden_1, hidden_2, hidden_3):\n",
    "    base_model = tf.keras.applications.ResNet50(include_top=False, weights=None, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = True\n",
    "    inputs = Input((224, 224, 3))\n",
    "    h = base_model(inputs, training=True)\n",
    "    h = GlobalAveragePooling2D()(h)\n",
    "\n",
    "    projection_1 = Dense(hidden_1)(h)\n",
    "    projection_1 = Activation(\"relu\")(projection_1)\n",
    "    projection_2 = Dense(hidden_2)(projection_1)\n",
    "    projection_2 = Activation(\"relu\")(projection_2)\n",
    "    projection_3 = Dense(hidden_3)(projection_2)\n",
    "\n",
    "    resnet_simclr = Model(inputs, projection_3)\n",
    "\n",
    "    return resnet_simclr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67828cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#from augmentation.gaussian_filter import GaussianBlur\n",
    "\n",
    "\n",
    "def get_negative_mask(batch_size):\n",
    "    # return a mask that removes the similarity score of equal/similar images.\n",
    "    # this function ensures that only distinct pair of images get their similarity scores\n",
    "    # passed as negative examples\n",
    "    negative_mask = np.ones((batch_size, 2 * batch_size), dtype=bool)\n",
    "    for i in range(batch_size):\n",
    "        negative_mask[i, i] = 0\n",
    "        negative_mask[i, i + batch_size] = 0\n",
    "    return tf.constant(negative_mask)\n",
    "\n",
    "\n",
    "def gaussian_filter(v1, v2):\n",
    "    k_size = int(v1.shape[1] * 0.1)  # kernel size is set to be 10% of the image height/width\n",
    "    gaussian_ope = GaussianBlur(kernel_size=k_size, min=0.1, max=2.0)\n",
    "    [v1, ] = tf.py_function(gaussian_ope, [v1], [tf.float32])\n",
    "    [v2, ] = tf.py_function(gaussian_ope, [v2], [tf.float32])\n",
    "    return v1, v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses.py\n",
    "import tensorflow as tf\n",
    "\n",
    "cosine_sim_1d = tf.keras.losses.CosineSimilarity(axis=1, reduction=tf.keras.losses.Reduction.NONE)\n",
    "cosine_sim_2d = tf.keras.losses.CosineSimilarity(axis=2, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "\n",
    "def _cosine_simililarity_dim1(x, y):\n",
    "    v = cosine_sim_1d(x, y)\n",
    "    return v\n",
    "\n",
    "\n",
    "def _cosine_simililarity_dim2(x, y):\n",
    "    # x shape: (N, 1, C)\n",
    "    # y shape: (1, 2N, C)\n",
    "    # v shape: (N, 2N)\n",
    "    v = cosine_sim_2d(tf.expand_dims(x, 1), tf.expand_dims(y, 0))\n",
    "    return v\n",
    "\n",
    "\n",
    "def _dot_simililarity_dim1(x, y):\n",
    "    # x shape: (N, 1, C)\n",
    "    # y shape: (N, C, 1)\n",
    "    # v shape: (N, 1, 1)\n",
    "    v = tf.matmul(tf.expand_dims(x, 1), tf.expand_dims(y, 2))\n",
    "    return v\n",
    "\n",
    "\n",
    "def _dot_simililarity_dim2(x, y):\n",
    "    v = tf.tensordot(tf.expand_dims(x, 1), tf.expand_dims(tf.transpose(y), 0), axes=2)\n",
    "    # x shape: (N, 1, C)\n",
    "    # y shape: (1, C, 2N)\n",
    "    # v shape: (N, 2N)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f37f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask to remove positive examples from the batch of negative samples\n",
    "negative_mask = get_negative_mask(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452af65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(xis, xjs, model, optimizer, criterion, temperature):\n",
    "    with tf.GradientTape() as tape:\n",
    "        zis = model(xis)\n",
    "        zjs = model(xjs)\n",
    "\n",
    "        # normalize projection feature vectors\n",
    "        zis = tf.math.l2_normalize(zis, axis=1)\n",
    "        zjs = tf.math.l2_normalize(zjs, axis=1)\n",
    "\n",
    "        l_pos = _dot_simililarity_dim1(zis, zjs)\n",
    "        l_pos = tf.reshape(l_pos, (BATCH_SIZE, 1))\n",
    "        l_pos /= temperature\n",
    "\n",
    "        negatives = tf.concat([zjs, zis], axis=0)\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for positives in [zis, zjs]:\n",
    "            l_neg = _dot_simililarity_dim2(positives, negatives)\n",
    "\n",
    "            labels = tf.zeros(BATCH_SIZE, dtype=tf.int32)\n",
    "\n",
    "            l_neg = tf.boolean_mask(l_neg, negative_mask)\n",
    "            l_neg = tf.reshape(l_neg, (BATCH_SIZE, -1))\n",
    "            l_neg /= temperature\n",
    "\n",
    "            logits = tf.concat([l_pos, l_neg], axis=1) \n",
    "            loss += criterion(y_pred=logits, y_true=labels)\n",
    "\n",
    "        loss = loss / (2 * BATCH_SIZE)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"project_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c0524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def train_simclr(model, dataset, optimizer, criterion,\n",
    "                 temperature=0.1, epochs=10):\n",
    "    step_wise_loss = []\n",
    "    epoch_wise_loss = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for image_batch in dataset:\n",
    "            a = data_augmentation(image_batch)\n",
    "            b = data_augmentation(image_batch)\n",
    "            \n",
    "            loss = train_step(a, b, model, optimizer, criterion, temperature)\n",
    "            step_wise_loss.append(loss)\n",
    "\n",
    "        epoch_wise_loss.append(np.mean(step_wise_loss))\n",
    "        wandb.log({\"nt_xentloss\": np.mean(step_wise_loss)})\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(\"epoch: {} loss: {:.3f}\".format(epoch + 1, np.mean(step_wise_loss)))\n",
    "\n",
    "    return epoch_wise_loss, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccfc570",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, \n",
    "                                                          reduction=tf.keras.losses.Reduction.SUM)\n",
    "decay_steps = 1000\n",
    "lr_decayed_fn = tf.keras.experimental.CosineDecay(\n",
    "    initial_learning_rate=0.1, decay_steps=decay_steps)\n",
    "optimizer = tf.keras.optimizers.SGD(lr_decayed_fn)\n",
    "\n",
    "import os\n",
    "ROOT_DIR = os.path.abspath(\"C:/Users/USERTEST/OneDrive - agu.edu.tr/Masaüstü/MASK_RCNN/\")\n",
    "resnet_simclr_2 = get_resnet_simclr(256, 128, 50)\n",
    "#resnet_simclr_2.summary()\n",
    "\n",
    "epoch_wise_loss, resnet_simclr  = train_simclr(resnet_simclr_2, train_ds, optimizer, criterion,\n",
    "                 temperature=0.1, epochs=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c255ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "filename = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"resnet_simclr.h5\"\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49083c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.makedirs('models/')          # Creating a directory\n",
    "resnet_simclr.save_weights('models/20211202-011118resnet_simclr_epoch50_batch16.h5')   # Saving model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
